# VLM-Enhanced Query Mode üñºÔ∏è

## ¬øQu√© es?

**VLM-Enhanced Query Mode** es una mejora al sistema ChatPDF que permite al modelo de IA **ver realmente las im√°genes originales** del PDF cuando respondes preguntas, en lugar de solo leer descripciones de texto.

## ¬øC√≥mo funciona?

### Antes (Caption-Only Mode)
1. Usuario sube PDF con im√°genes
2. Sistema genera descripciones textuales (captions) de las im√°genes
3. Durante el chat, el modelo solo lee las descripciones
4. **Limitaci√≥n**: El modelo no puede ver detalles visuales espec√≠ficos

### Ahora (VLM-Enhanced Mode)
1. Usuario sube PDF con im√°genes
2. Sistema guarda las im√°genes permanentemente en `uploads/images/{pdf_id}/`
3. Sistema genera captions Y guarda referencias en BD (tabla `pdf_images`)
4. Durante el chat:
   - Si detecta `[IMAGE_CAPTIONS]` en el contexto
   - **Carga las im√°genes originales desde disco**
   - **Env√≠a im√°genes + texto al modelo de visi√≥n (GPT-4o-mini)**
   - El modelo "ve" realmente los gr√°ficos, diagramas, tablas, etc.

## Cambios Implementados

### 1. Base de Datos
**Nueva tabla**: `pdf_images`
```sql
CREATE TABLE pdf_images (
    id SERIAL PRIMARY KEY,
    pdf_id INTEGER REFERENCES pdfs(id) ON DELETE CASCADE,
    image_path TEXT NOT NULL,
    page_number INTEGER,
    created_at TIMESTAMPTZ DEFAULT NOW()
);
```

### 2. Extracci√≥n de Im√°genes
**Funci√≥n modificada**: `extract_images_from_pdf(file_path, pdf_id=None)`
- Ahora acepta `pdf_id` opcional
- Si se proporciona `pdf_id`, guarda im√°genes permanentemente en:
  - `backend/uploads/images/{pdf_id}/page_1.png`
  - `backend/uploads/images/{pdf_id}/page_2.png`
  - etc.
- Retorna lista de tuplas: `[(image_path, page_number), ...]`

### 3. Upload de PDFs
**Endpoints modificados**: `/upload_pdf/` y `/upload_pdfs/`
- Crea entrada de PDF **PRIMERO** para obtener `pdf_id`
- Extrae im√°genes con `pdf_id` para guardarlas permanentemente
- Registra cada imagen en tabla `pdf_images`:
  ```python
  INSERT INTO pdf_images (pdf_id, image_path, page_number) 
  VALUES (pdf_id, '/path/to/image', page_num)
  ```
- Las im√°genes **NO se borran** despu√©s del procesamiento

### 4. Endpoint de Chat
**Endpoint modificado**: `/chat/`
- Detecta si contexto contiene `[IMAGE_CAPTIONS]`
- Si es as√≠, activa **VLM-Enhanced Mode**:
  ```python
  if "[IMAGE_CAPTIONS]" in context:
      # Cargar im√°genes del PDF desde BD
      SELECT image_path, page_number FROM pdf_images WHERE pdf_id = ?
      
      # Para OpenAI: enviar im√°genes + texto al modelo
      messages = [
          {"role": "system", "content": system_prompt},
          {"role": "user", "content": [
              {"type": "text", "text": query + context},
              {"type": "image_url", "image_url": {"url": "data:image/png;base64,..."}}
          ]}
      ]
  ```
- Usa modelo `gpt-4o-mini` (capaz de procesar visi√≥n)
- Limita a primeras 5 im√°genes para evitar exceder l√≠mites de tokens
- **Fallback autom√°tico** a modo text-only si hay errores

## Ventajas

‚úÖ **Precisi√≥n mejorada**: El modelo ve los detalles visuales reales  
‚úÖ **Respuestas m√°s espec√≠ficas**: Puede leer valores exactos de gr√°ficos, tablas, etc.  
‚úÖ **Mejor experiencia**: Usuario puede preguntar "¬øqu√© dice el gr√°fico de la p√°gina 3?" y el modelo lo ve  
‚úÖ **Retrocompatibilidad**: Si no hay im√°genes, funciona como antes  
‚úÖ **Fallback autom√°tico**: Si VLM-enhanced falla, usa captions como antes  

## Uso

### Variables de Entorno
```bash
OPENAI_API_KEY=sk-...           # Requerido para VLM-Enhanced Mode
ENABLE_VISION_CAPTIONS=true     # Auto-activado si OPENAI_API_KEY presente
ENABLE_OCR=true                 # OCR adicional con Tesseract
```

### Ejemplo de Pregunta
```
Usuario: "¬øQu√© valores muestra el gr√°fico de barras en la p√°gina 3?"

Sistema (VLM-Enhanced):
1. Recupera chunks relevantes (incluyen [IMAGE_CAPTIONS])
2. Detecta marcador [IMAGE_CAPTIONS]
3. Carga imagen de p√°gina 3 desde disco
4. Env√≠a imagen + contexto a GPT-4o-mini
5. Modelo "ve" el gr√°fico y responde con valores espec√≠ficos
```

## L√≠mites y Consideraciones

‚ö†Ô∏è **L√≠mite de im√°genes**: M√°ximo 5 im√°genes por query (evitar exceder tokens)  
‚ö†Ô∏è **Tama√±o de imagen**: Im√°genes >100KB base64 se saltan autom√°ticamente  
‚ö†Ô∏è **Costo**: VLM-Enhanced usa m√°s tokens que text-only (m√°s costoso)  
‚ö†Ô∏è **Latencia**: Enviar im√°genes toma m√°s tiempo que solo texto  
‚ö†Ô∏è **Fallback**: Si imagen no existe o hay error, usa captions como antes  

## Logs de Debug

Busca estos mensajes en los logs:
```
INFO:main:Detected IMAGE_CAPTIONS in context, loading images for VLM-enhanced mode
INFO:main:Loaded 6 images for pdf_id=123
INFO:main:Using VLM-enhanced mode with 6 images
INFO:main:Added image from page 1 to VLM request
INFO:main:VLM-enhanced response generated successfully
```

Si VLM-enhanced falla:
```
WARNING:main:VLM-enhanced mode failed: ...
INFO:main:Using text-only mode (no images or no API key)
```

## Pr√≥ximos Pasos

üîÑ **Context-Aware Captions**: Incluir texto circundante al generar captions  
üîÑ **Ollama VLM Support**: Agregar soporte para modelos de visi√≥n en Ollama  
üîÑ **Selective Image Loading**: Solo cargar im√°genes mencionadas en query  
üîÑ **Image Compression**: Comprimir im√°genes antes de enviar (reducir tokens)  

## Testing

Para probar VLM-Enhanced Mode:
1. Sube un PDF con gr√°ficos/tablas/diagramas
2. Verifica que se crearon archivos en `backend/uploads/images/{pdf_id}/`
3. Haz una pregunta espec√≠fica sobre una imagen
4. Revisa logs para confirmar "Using VLM-enhanced mode"
5. Compara respuesta vs. modo caption-only

---
**Implementado**: 18 de Octubre, 2025  
**Autor**: GitHub Copilot  
**Versi√≥n**: 1.0
